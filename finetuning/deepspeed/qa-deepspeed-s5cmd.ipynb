{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing Libraries and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install huggingface specific libraries\n",
    "! pip install transformers datasets evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Common libraries' imports\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from time import gmtime, strftime\n",
    "from random import randrange\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check versions\n",
    "from platform import python_version\n",
    "import torch\n",
    "import sagemaker\n",
    "import transformers\n",
    "import datasets\n",
    "print('Pytorch version: ', torch.__version__)\n",
    "print('Python version: ', python_version())\n",
    "print('Sagemaker version: ', sagemaker.__version__)\n",
    "print('Transformers version: ', transformers.__version__)\n",
    "print('Datasets version: ', datasets.__version__)\n",
    "\n",
    "# Pytorch version:  1.13.1\n",
    "# Python version:  3.9.15\n",
    "# Sagemaker version:  2.146.0\n",
    "# Transformers version:  4.29.2\n",
    "# Datasets version:  2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sagemaker specific imports\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n",
    "\n",
    "# Huggingface specific imports\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generic initilizations\n",
    "model_id = 'google/flan-t5-xxl' \n",
    "workspace_bucket_name = 'gupshup-ml' # This s3 bucket is for storing datasets used for training.\n",
    "s3_prefix = 'deepspeed' # s3 prefix at which train and test dataets will be saved. Ex - s3://gupshup-ml/deepspeed/train\n",
    "model_name = model_id.split('/')[1]\n",
    "save_model_s3_path = f's3://{workspace_bucket_name}/{s3_prefix}/{model_name}-deepspeed/' # s3 path where model artifacts gets stored (Used when trying to save using s5cmd)\n",
    "experiment_name = model_name + '-finetuning'\n",
    "job_name = 'qa-deepspeed-' + model_name\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 3                           # number of training epochs\n",
    "per_device_train_batch_size = 5      # batch size for training\n",
    "per_device_eval_batch_size = 5       # batch size for evaluation\n",
    "gradient_accumulation_steps = 64\n",
    "learning_rate = float('1e-4')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sagemaker specific\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f'Sagemaker Role ARN: {role}')\n",
    "print(f'Sagemaker Bucket: {sess.default_bucket()}')\n",
    "print(f'Sagemaker Session Region: {sess.boto_region_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load csv as a pandas dataframe\n",
    "train_path = '../others/data/full_data/train/sniper_faq_session_train.csv'\n",
    "train = pd.read_csv(train_path)\n",
    "train['id'] = train.index\n",
    "train = train[['id', 'input_text', 'output_text']]\n",
    "print('Train:', train.shape)\n",
    "\n",
    "test_path = '../others/data/full_data/test/sniper_faq_session_test.csv'\n",
    "test = pd.read_csv(test_path)\n",
    "test['id'] = test.index\n",
    "test = test[['id', 'input_text', 'output_text']]\n",
    "test.dropna(inplace=True)\n",
    "print('Test:', test.shape)\n",
    "\n",
    "# Create Dataset from pandas dataframes\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "dataset\n",
    "\n",
    "# train = 6673, test = 1668 samples\n",
    "# train.info(memory_usage='deep') # memory usage: 26.6 MB\n",
    "# test.info(memory_usage='deep') # memory usage: 6.6 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer for a chosen model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(f'Model input names: {tokenizer.model_input_names}')\n",
    "print(f'Model max length: {tokenizer.model_max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Data Processing\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset['train'], dataset['test']]).map(lambda x: tokenizer(x['input_text'], truncation=True), batched=True, remove_columns=['input_text', 'output_text'])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs['input_ids']])\n",
    "print(f'Max source length: {max_source_length}')\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.'\n",
    "tokenized_targets = concatenate_datasets([dataset['train'], dataset['test']]).map(lambda x: tokenizer(x['output_text'], truncation=True), batched=True, remove_columns=['input_text', 'output_text'])\n",
    "max_target_length = max([len(x) for x in tokenized_targets['input_ids']])\n",
    "print(f'Max target length: {max_target_length}')\n",
    "\n",
    "def preprocess_function(sample, padding='max_length'):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = sample['input_text']\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample['output_text'], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == 'max_length':\n",
    "        labels['input_ids'] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels['input_ids']\n",
    "        ]\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['id', 'input_text', 'output_text'])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the train and test datasets to s3\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "training_input_path = f's3://{workspace_bucket_name}/{s3_prefix}/train'\n",
    "print(f'Training input path: {training_input_path}')\n",
    "tokenized_dataset['train'].save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "test_input_path = f's3://{workspace_bucket_name}/{s3_prefix}/test'\n",
    "print(f'Test input path: {test_input_path}')\n",
    "tokenized_dataset['test'].save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deepspeed related parameters for training job\n",
    "deepspeed_parameters = {\n",
    "  'deepspeed': './configs/ds_flan_t5_z3_offload', # path to deepspeed config file\n",
    "  'training_script': './scripts/qa-deepspeed-s5cmd.py' # path to real training script, not entrypoint\n",
    "}\n",
    "\n",
    "hyperparameters, which are passed into the training job\n",
    "training_hyperparameters={\n",
    "  'model_id': model_id,                                        # pre-trained model\n",
    "  'epochs': epochs,                                            # number of training epochs\n",
    "  'per_device_train_batch_size': per_device_train_batch_size,  # batch size for training\n",
    "  'per_device_eval_batch_size': per_device_eval_batch_size,    # batch size for evaluation\n",
    "  'gradient_accumulation_steps': gradient_accumulation_steps,  # gradient accumulation steps\n",
    "  'learning_rate': learning_rate,                              # learning rate used during training\n",
    "  'generation_max_length': max_target_length,                  # max length of generated summary\n",
    "  'save_model_s3_path': save_model_s3_path\n",
    "}\n",
    "\n",
    "print('Hyperparameters: \\n', json.dumps(training_hyperparameters, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'launcher.py',     # deepspeed launcher script\n",
    "    source_dir           = '.',               # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.p4d.24xlarge', # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # IAM role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.17',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.10',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py38',            # the python version used in the training job\n",
    "    hyperparameters      = {\n",
    "      **training_hyperparameters,\n",
    "      **deepspeed_parameters\n",
    "    },                                        # the hyperparameter used for running the training job\n",
    "    keep_alive_period_in_seconds = 1800,      # useful parameter when trying small changes right after failed job status. NOTE: This will keep the instance alive. Lookout for costs\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'train': training_input_path,\n",
    "    'test' : test_input_path,\n",
    "}\n",
    "print(json.dumps(data, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True) # Can also go with logs='None' parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator.hyperparameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
