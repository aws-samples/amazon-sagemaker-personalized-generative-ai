{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d54fa75-4755-4c69-a2aa-ca56a6c95e27",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing Libraries and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce16bea-3213-431a-8c6d-06de07a6160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install huggingface specific libraries\n",
    "! pip install transformers datasets evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b884a-f32f-417e-8271-6c43c6152fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Common libraries\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# Sagemaker specific\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n",
    "\n",
    "# Huggingface specific\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a9a62-a20b-45c6-ab94-e421f68692b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic initializations\n",
    "model_path = 'google/flan-t5-xxl'\n",
    "workspace_bucket_name = 'gupshup-ml'\n",
    "s3_prefix = 'peft'\n",
    "model_name = model_path.split('/')[1]\n",
    "save_model_s3_path = f's3://{workspace_bucket_name}/{s3_prefix}/{model_name}-peft/'\n",
    "base_job_name = f'qa-peft-{model_name}'\n",
    "experiment_name = f'qa-peft-{model_name}'\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 1                           # number of training epochs\n",
    "per_device_train_batch_size = 5      # batch size for training\n",
    "gradient_accumulation_steps = 64     # gradient accumulation steps for training\n",
    "learning_rate = float('1e-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe336c-0652-4bd7-aaa1-eb7a302f1143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sagemaker specific\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f'Sagemaker Role ARN: {role}')\n",
    "print(f'Sagemaker Bucket: {sess.default_bucket()}')\n",
    "print(f'Sagemaker Session Region: {sess.boto_region_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12ade2-70e9-4abf-ad17-ff088c30c4ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac25fcc-9feb-4f6c-b18c-2a8a7036f3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load csv as a pandas dataframe\n",
    "train_path = '../others/data/full_data/train/sniper_faq_session_train.csv'\n",
    "train = pd.read_csv(train_path)\n",
    "train['id'] = train.index\n",
    "train = train[['id', 'input_text', 'output_text']]\n",
    "print('Train:', train.shape)\n",
    "\n",
    "test_path = '../others/data/full_data/test/sniper_faq_session_test.csv'\n",
    "test = pd.read_csv(test_path)\n",
    "test['id'] = test.index\n",
    "test = test[['id', 'input_text', 'output_text']]\n",
    "test.dropna(inplace=True)\n",
    "print('Test:', test.shape)\n",
    "\n",
    "# Create Dataset from pandas dataframes\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "dataset\n",
    "\n",
    "# train = 6673, test = 1668 samples\n",
    "# train.info(memory_usage='deep') # memory usage: 26.6 MB\n",
    "# test.info(memory_usage='deep') # memory usage: 6.6 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcdd0a-d486-4f39-ada8-59b3f4591852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer for a chosen model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(f'Model input names: {tokenizer.model_input_names}')\n",
    "print(f'Model max length: {tokenizer.model_max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1f070-f260-4226-a842-7a2a70c7685e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Data processing\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset['train'], dataset['test']]).map(lambda x: tokenizer(x['input_text'], truncation=True), batched=True, remove_columns=['input_text', 'output_text'])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs['input_ids']]\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f'Max source length: {max_source_length}')\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.'\n",
    "tokenized_targets = concatenate_datasets([dataset['train'], dataset['test']]).map(lambda x: tokenizer(x['output_text'], truncation=True), batched=True, remove_columns=['input_text', 'output_text'])\n",
    "target_lenghts = [len(x) for x in tokenized_targets['input_ids']]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f'Max target length: {max_target_length}')\n",
    "\n",
    "def preprocess_function(sample,padding='max_length'):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = sample['input_text']\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample['output_text'], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == 'max_length':\n",
    "        labels['input_ids'] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels['input_ids']\n",
    "        ]\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['id', 'input_text', 'output_text'])\n",
    "print('Keys of tokenized dataset:', list(tokenized_dataset['train'].features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c5a50-7690-4709-8a73-33bf0d29d8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the train and test datasets to s3\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "training_input_path = f's3://{workspace_bucket_name}/{s3_prefix}/train'\n",
    "print(f'Training input path: {training_input_path}')\n",
    "tokenized_dataset['train'].save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "test_input_path = f's3://{workspace_bucket_name}/{s3_prefix}/test'\n",
    "print(f'Test input path: {test_input_path}')\n",
    "tokenized_dataset['test'].save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d7af4-5c44-4408-926e-35da87608528",
   "metadata": {},
   "source": [
    "# Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc45636-e7fa-4f63-9f41-5920c332f128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    'model_id': model_path,\n",
    "    'learning_rate': learning_rate,\n",
    "    'per_device_train_batch_size': per_device_train_batch_size,\n",
    "    'gradient_accumulation_steps': gradient_accumulation_steps,\n",
    "    'epochs': epochs,\n",
    "    'save_model_s3_path': save_model_s3_path\n",
    "}\n",
    "print('Hyperparameters: \\n', json.dumps(hyperparameters, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f65921-3442-4229-8d33-f08d3ce10e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hf_estimator = HuggingFace(\n",
    "        entry_point          = './scripts/qa-peft-s5cmd.py',      # training script filename \n",
    "        source_dir           = '.',                               # training script source-dir\n",
    "        instance_type        = 'ml.p4d.24xlarge',                 # instances type used for the training job  \n",
    "        instance_count       = 1,                                 # the number of instances used for training\n",
    "        base_job_name        = base_job_name,                     # the name of the training job\n",
    "        role                 = role,                              # IAM role used in training job to access AWS ressources, e.g. S3\n",
    "        transformers_version = '4.26.0',                          # the transformers version used in the training job\n",
    "        pytorch_version      = '1.13.1',                          # the pytorch_version version used in the training job\n",
    "        py_version           = 'py39',                            # the python version used in the training job\n",
    "        hyperparameters      = hyperparameters,                   # the hyperparameter used for running the training job\n",
    "        volume_size          = 300,                               # the size of the EBS volume in GB\n",
    "        disable_profiler     = True, \n",
    "        debugger_hook_config = False,\n",
    "        keep_alive_period_in_seconds = 1800,                      # useful parameter when trying small changes right after failed job status. NOTE: This will keep the instance alive. Lookout for costs\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'train': training_input_path\n",
    "}\n",
    "print(json.dumps(data, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc02bc4-b7e8-4bb6-9e98-f436473382d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# starting the train job with our uploaded datasets as input\n",
    "hf_estimator.fit(data, wait=True) # Can also go with logs='None' parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c611ceb-c23f-4f30-9155-4bf806780451",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_estimator.hyperparameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
