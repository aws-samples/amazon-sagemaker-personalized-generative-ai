{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importing libraries and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install huggingface specific libraries\n",
    "! pip install transformers datasets evaluate --quiet\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Common libraries\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from time import gmtime, strftime\n",
    "from random import randrange\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check versions\n",
    "from platform import python_version\n",
    "import torch\n",
    "import sagemaker\n",
    "import transformers\n",
    "import datasets\n",
    "print('Pytorch version: ', torch.__version__)\n",
    "print('Python version: ', python_version())\n",
    "print('Sagemaker version: ', sagemaker.__version__)\n",
    "print('Transformers version: ', transformers.__version__)\n",
    "print('Datasets version: ', datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sagemaker specific imports\n",
    "import boto3\n",
    "from sagemaker.pytorch import PyTorch\n",
    "# from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n",
    "\n",
    "# Huggingface specific imports\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = 'google/flan-t5-xxl' \n",
    "workspace_bucket_name = 'gupshup-ml' # This s3 bucket is for storing datasets used for training.\n",
    "s3_prefix = 'smp' # s3 prefix at which train and test dataets will be saved. Ex - s3://gupshup-ml/smp/train\n",
    "model_name = model_path.split('/')[1]\n",
    "save_model_s3_path = f's3://gupshup-ml/model-artifacts/{model_name}-smp/' # s3 path where model artifacts gets stored (Used when trying to save using s5cmd)\n",
    "experiment_name = f'qa-smp-{model_name}'\n",
    "\n",
    "epochs = 1                           # number of training epochs\n",
    "per_device_batch_size = 5            # batch size for training and evaluation\n",
    "gradient_accumulation_steps = 64     # gradient accumulation steps for training\n",
    "learning_rate = float('1e-4') \n",
    "# optim = 'adamw_torch_xla'\n",
    "pipeline_parallel_degree = 1\n",
    "sharded_data_parallel_degree = 16 \n",
    "partitions = 1\n",
    "processes_per_host = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load csv as a pandas dataframe\n",
    "train_path = '../others/data/full_data/train/sniper_faq_session_train.csv'\n",
    "train = pd.read_csv(train_path)\n",
    "train['id'] = train.index\n",
    "train = train[['id', 'input_text', 'output_text']]\n",
    "print('Train:', train.shape)\n",
    "\n",
    "test_path = '../others/data/full_data/test/sniper_faq_session_test.csv'\n",
    "test = pd.read_csv(test_path)\n",
    "test['id'] = test.index\n",
    "test = test[['id', 'input_text', 'output_text']]\n",
    "test.dropna(inplace=True)\n",
    "print('Test:', test.shape)\n",
    "\n",
    "# Create Dataset from pandas dataframes\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "dataset\n",
    "\n",
    "# train = 6673, test = 1668 samples\n",
    "# train.info(memory_usage='deep') # memory usage: 26.6 MB\n",
    "# test.info(memory_usage='deep') # memory usage: 6.6 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer for a chosen model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(f'Model input names: {tokenizer.model_input_names}')\n",
    "print(f'Model max length: {tokenizer.model_max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Data processing\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset['train'], dataset['test']]).map(lambda x: tokenizer(x['input_text'], truncation=True), batched=True, remove_columns=['input_text', 'output_text'])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs['input_ids']]\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f'Max source length: {max_source_length}')\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.'\n",
    "tokenized_targets = concatenate_datasets([dataset['train'], dataset['test']]).map(lambda x: tokenizer(x['output_text'], truncation=True), batched=True, remove_columns=['input_text', 'output_text'])\n",
    "target_lenghts = [len(x) for x in tokenized_targets['input_ids']]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f'Max target length: {max_target_length}')\n",
    "\n",
    "def preprocess_function(sample,padding='max_length'):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = sample['input_text']\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample['output_text'], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == 'max_length':\n",
    "        labels['input_ids'] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels['input_ids']\n",
    "        ]\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['id', 'input_text', 'output_text'])\n",
    "print('Keys of tokenized dataset:', list(tokenized_dataset['train'].features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the train and test datasets to s3\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "training_input_path = f's3://{workspace_bucket_name}/{s3_prefix}/train'\n",
    "print(f'Training input path: {training_input_path}')\n",
    "tokenized_dataset['train'].save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "test_input_path = f's3://{workspace_bucket_name}/{s3_prefix}/test'\n",
    "print(f'Test input path: {test_input_path}')\n",
    "tokenized_dataset['test'].save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/opt/ml/checkpoints\"\n",
    "checkpoint_s3_path = \"s3://\" + workspace_bucket_name + \"/flant5-checkpoints\"\n",
    "print(checkpoint_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 rm --recursive $checkpoint_s3_path # Not for first run - its for subsequent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "hyperparameters = {\n",
    "    'model_id': model_path,\n",
    "    'learning_rate': learning_rate,\n",
    "    'per_device_train_batch_size': per_device_batch_size,\n",
    "    'gradient_accumulation_steps': gradient_accumulation_steps,\n",
    "    'per_device_eval_batch_size': per_device_batch_size,\n",
    "    'epochs': epochs,\n",
    "    'save_model_s3_path': save_model_s3_path,\n",
    "    'checkpoint_dir': \"/opt/ml/checkpoints\",\n",
    "    'max_train_steps': 500,\n",
    "    \n",
    "    'pipeline_parallel_degree': pipeline_parallel_degree,\n",
    "    'sharded_data_parallel_degree': sharded_data_parallel_degree,\n",
    "    'partitions': partitions, # NOTE: Sharded data parallelism currently is not compatible with pipeline parallelism or optimizer state sharding. To activate sharded data parallelism, turn off optimizer state sharding and set the pipeline parallel degree to 1.\n",
    "    # 'tensor_parallel_degree': 1,\n",
    "    'processes_per_host': processes_per_host,\n",
    "}\n",
    "print('Hyperparameters: \\n', json.dumps(hyperparameters, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {                        \n",
    "        \"pipeline_parallel_degree\": hyperparameters['pipeline_parallel_degree'],     \n",
    "        \"ddp\": True,\n",
    "        \"sharded_data_parallel_degree\": hyperparameters['sharded_data_parallel_degree'],              \n",
    "        \"partitions\": hyperparameters['partitions'],\n",
    "        \"bf16\":True,\n",
    "        \"skip_tracing\": True,\n",
    "        \n",
    "        # NOTE: To enable Tensor Parallelism\n",
    "        # \"tensor_parallel_degree\": hyperparameters['tensor_parallel_degree'],\n",
    "        # \"prescaled_batch\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,\n",
    "    \"processes_per_host\" : hyperparameters['processes_per_host'],\n",
    "    # Below is to debug parallel Open MPI processes. TODO: Test the impact on logging.\n",
    "    # \"custom_mpi_options\": \"-verbose --mca orte_base_help_aggregate 0 \",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point                  = \"./scripts/qa-peft-smp.py\",\n",
    "    source_dir                   = \".\",\n",
    "    role                         = role,\n",
    "    framework_version            = \"1.13.1\",\n",
    "    py_version                   = \"py39\", \n",
    "    \n",
    "    # base_job_name                = 'peft-smp-p4',\n",
    "    # instance_count               = 1,\n",
    "    # instance_type                = \"ml.p4d.24xlarge\",\n",
    "    \n",
    "    base_job_name                = 'peft-smp-p3-24xl',\n",
    "    instance_count               = 2,\n",
    "    instance_type                = \"ml.p3dn.24xlarge\",\n",
    "    \n",
    "    # base_job_name                = 'peft-sdp-p3-24xl',\n",
    "    # instance_count               = 1,\n",
    "    # instance_type                = \"ml.p3dn.24xlarge\",\n",
    "    \n",
    "    # base_job_name                = 'peft-smp-p3-16xl',\n",
    "    # instance_count               = 1,\n",
    "    # instance_type                = \"ml.p3.16xlarge\", # Don't have SL\n",
    "    \n",
    "    # base_job_name                = 'peft-smp-g5-48xl',\n",
    "    # instance_count               = 1,\n",
    "    # instance_type                = \"ml.g5.48xlarge\",\n",
    "    \n",
    "    # base_job_name                = 'peft-smp-g5-12xl',\n",
    "    # instance_count               = 2,\n",
    "    # instance_type                = \"ml.g5.12xlarge\",\n",
    "    \n",
    "    # base_job_name                = 'peft-smp-g5-16xl',\n",
    "    # instance_count               = 4,\n",
    "    # instance_type                = \"ml.g5.16xlarge\",\n",
    "    \n",
    "    # base_job_name                = 'peft-smp-g4dn-12xl',\n",
    "    # instance_count               = 4,\n",
    "    # instance_type                = \"ml.g4dn.12xlarge\",\n",
    "    \n",
    "    hyperparameters              = hyperparameters,\n",
    "    checkpoint_local_path        = checkpoint_dir,   \n",
    "    checkpoint_s3_uri            = checkpoint_s3_path,\n",
    "    disable_profiler             = True,\n",
    "    keep_alive_period_in_seconds = 1800, # TODO: Change it to a reasonable value as its gonna add to total billing cost.\n",
    "    debugger_hook_config         = False,\n",
    "    \n",
    "    # distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "    distribution = {\n",
    "                    \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "                    \"mpi\": mpi_options\n",
    "                   }\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'train': training_input_path,\n",
    "    'test' : test_input_path,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(data, wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2df149412efc1526e813459d121195dcad0cc0c344007149632d30b7359a266e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
